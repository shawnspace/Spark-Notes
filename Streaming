# Programming Model
先定义图，再开启stream，执行计算

# StreamingContext
先create SparkContext，再从中create StreamingContext，是entry point

# Discretized Streams (DStreams)
1. 代表了一个data stream
2. 内部是RDD的序列：spark将data stream按时间interval来划分，每一个interval的数据作为一个batch，存到一个RDD里。n个interval就有n个RDD
3. 所有的transformation 和 action 都可以对DStream使用，内部都会转化到对每个interval的RDD使用

# Input DStreams and Receivers
1. 可以从Basic source和 advance source创造 Input DStream
2. 除了Filesystem中创造的 input source外，每一个input stream都有一个receiver。

# Transformation operations
1. 支持大部分RDD中常见的transformation
2. UpdateStateByKey Operation：传递一个update state的函数，在每一个batch中，会调用该函数进行state的更新
3. Transform Operation: 可以使用DStream API中没有出现的RDD op到stream上
4. Window Operations：可以把不同batch的RDD聚合到一个window中
5. Join Operations: join stream with other stream or dataset

# Output Operations
1. 输出DStream到database或者file system
2. 类似于Action operation：会触发该DStream的所有Transformation计算

# Cache
1. persist() method on a DStream will automatically persist every RDD of that DStream in memory
2. For window-based operations and state-based operations, DStreams generated by such operations are automatically persisted in memory, without the developer calling persist().



