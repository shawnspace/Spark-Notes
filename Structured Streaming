# Overview
针对结构化数据的streaming API，内部计算是在spark SQL engine上完成。
整个data stream就是一个DataFrame，其代码和在static dataframe上的代码基本一致。
目前是micro-batch processing模式，就是内部把data stream分成不同的batch。Continuous processing就是每来一个数据就进行计算，而不用等到一个trigger，但还在实验中，其延迟会低于micro-batch processing

# Basic Concept
1. 整个data stream就是一个DataFrame，只不过是unbounded的table。每个数据是一个row
2. 在data stream上的计算图，会产生一个新的DataFrame，叫result table。每过一个time interval，更新一次result table里的内容：可能是新增row，也可能是修改已经存在的row
3. Output：可以把result table输出到外部存储中

# Input Streaming
1. 可以从file，Socket和Kafka source中建立
2. 使用Dataset.readStream()来建立，再使用load来得到一个DataFrame，表示stream。

# Transformation Operations
1. Selection, Projection, Aggregation等SQL的operation，又叫untyped operation
2. Window Operation: 使用window来构建window index。然后使用grouping和aggregation来操作。常用来处理event-time data。
  2.1 处理late data：使用watermarking来指定，经过多久时间后的才到的数据会被drop
3. Join operation: 将该stream和a static Dataset/DataFrame 或者 another streaming Dataset/DataFrame进行join
4. Deduplication：用一个column来当ID，进行去重。也可以指定watermark

# Output Streaming
1. 要启动streaming，需要用Dataset.writeStream()来指定output路径和格式，再用start启动
2. 用start启动streaming后，得到一个StreamingQuery的handle
3. 要指定output mode和output sink
  3.1 Foreach and ForeachBatch sink：可以对output streaming使用arbitrary operations and writing logic
4. Trigger：设置streaming data processing的时间

# Managing Streaming Query
1. 一些管理单个streamingQuery的操作
2. 一个sparksession中，多个stream可以并行的在cluster上运行，用sparkSession.streams()来得到StreamingQueryManager

# Monitoring Streaming Queries
1. 查看上一个trigger后的整体执行情况：streamingQuery.lastProgress()
2. 查看当前正在做什么：streamingQuery.status() 

# Recover from Checkpoint
1. 可以在writeStream开始前指定checkpoint path。spark会将当前进度保存到checkpoint中，以便发生failure时恢复计算


