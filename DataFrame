# SparkSession
用SparkSession.builder这个静态成员类来构造一个新的SparkSession（工厂模式）

# DataFrames
构造DataFrames
With a SparkSession, applications can create DataFrames from an existing RDD, from a Hive table, or from Spark data sources
1.From an RDD:
1.1 当已知DataFrames的schema时：先创建Row，在Row中通过kwargs来指定column的名字，the types are inferred by sampling the whole dataset
people = parts.map(lambda p: Row(name=p[0], age=int(p[1]))) # create RDD of Row
schemaPeople = spark.createDataFrame(people)
1.2 当不知道DataFrames的schema，或者不想一个个去hardcode出来时：Create an RDD of tuples or lists；Create the schema represented by a StructType；Apply the schema to the RDD
people = parts.map(lambda p: (p[0], p[1].strip())) #Create an RDD of tuples or lists
schemaString = "name age"
fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]
schema = StructType(fields) #Create the schema
schemaPeople = spark.createDataFrame(people, schema) #Apply the schema to the RDD

2.From data sources
manually specify the data source that will be used along with any extra options
JSON: df = spark.read.load("examples/src/main/resources/people.json", format="json")
CSV: df = spark.read.load("examples/src/main/resources/people.csv",format="csv", sep=":", inferSchema="true", header="true")

Untyped Operations
不同于RDD，DataFrames的很多操作是untyped的，也就是可以对不同类型的columns一起进行操作
Access a column： either by attribute (df.age) or by indexing (df['age'])
Print the schema in a tree format：df.printSchema()
Select only the "name" column： df.select("name").show()
Select people older than 21：df.filter(df['age'] > 21).show()
Count people by age：df.groupBy("age").count().show()

Aggregation
The built-in DataFrames aggregations such as count(), countDistinct(), avg(), max(), min()

存储DataFrames
普通储存：save
存为Hive persistent tables：saveAsTable
bucket and sort: df.write.bucketBy(42, "name").sortBy("age").saveAsTable("people_bucketed")
partition: df.write.partitionBy("favorite_color").format("parquet").save("namesPartByColor.parquet")

Cache DataFrames in memory
df.cache()

Conversion to/from Pandas
spark.conf.set("spark.sql.execution.arrow.enabled", "true")
df = spark.createDataFrame(pdf) # Create a Spark DataFrame from a Pandas DataFrame using Arrow
result_pdf = df.select("*").toPandas() # Convert the Spark DataFrame back to a Pandas DataFrame using Arrow


