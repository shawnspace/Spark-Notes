硬件层面
cluster
node
core：一个core就是一个cpu之类的计算单位，node上一般有好多core

OS层面
ResourceManager:资源管理
Client：提交Application并监控Application
ApplicationMaster：向ResourceManager申请Container资源运行Executor进程，并启动Driver
Driver：向ApplicationMaster申请资源，并进行任务调度
Executor：计算任务并上报给driver，每个Executor对应一个JVM

Partition: 对数据的分区，每个partition对应一个task
Task：Each task is run as a 线程 in executor。每个task有自己的一个closure：属于自己的varible或method


API层面
SparkContext
Entry point before 2.0
Create RDD and manipulate RDD API
For every other APIs, different contexts were required
one and only one SparkContext on a single JVM

SparkSession
New entry point after 2.0
Includes all the APIs in different contexts - Spark Context, SQL Context, Streaming Context, Hive Context.
Can have zero, two or more SparkSessions in a single Spark application.
Have at least and often only one SparkSession in a Spark SQL application
SparkSession is a mere wrapper around SparkContext to offer Spark SQL's structured/SQL features on top of Spark Core's RDDs.


RDD: a collection of elements partitioned across the nodes of the cluster，可以进行并行化操作。一个machine上有几个partition，每个partition对应一个task，每个task可以由好几个core去执行。
DataFrame：基于RDD提供的，API上比RDD更容易使用，更适合工业界的实际场景，性能往往更好
Dataset：比DataFrame更高级的API，DataFrame 等于 Dataset[row]

Shared variables：Spark runs a function in parallel as a set of tasks on different nodes, it ships a copy of each variable used in the function to each task
第一类：broadcast variables，which can be used to cache a value in memory on all nodes
第二类：accumulators, which are variables that are only “added” to

Spark SQL
Spark专门用来处理structured data的engine（不光是执行SQL语句）

Spark Streaming
用来处理RDD streaming的，执行在Spark engine上

Spark Structured Streaming
用来处理结构化数据 streaming的，执行在Spark SQL engine上

Library结构
pyspark
  RDD, Broadcast, Accumulator等关于RDD的class
  pyspark.sql SQL的subpackage，包括了Structured Streaming的功能（跟着DataFrame）
  pyspark.streaming DStream的subpackage
  pyspark.ml DataFrame-based machine learning APIs的subpackage
  pyspark.mllib RDD based API的subpackage


