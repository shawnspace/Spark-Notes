硬件层面
cluster
node
core：一个core就是一个cpu之类的计算单位

OS层面
Task：Each task is run as a process thread in executor JVM。每个task有自己的一个closure：属于自己的varible或method
Executor：Each Executor hosts multiple tasks during processing of a stage and the subsequent jobs associated with the code that was executed。
Partition: 对数据的分区
JVM：每个node上有一个JVM，一个JVM里有好几个executor。

关系：一个node上有几个partition，每个partition对应一个task，一个Executor执行好几个task，每个task可以由好几个core去执行。

API层面
每一个spark程序都是由 a driver program that runs the user’s main function and executes various parallel operations on a cluster。

SparkContext
Entry point before 2.0
Create RDD and manipulate RDD API
For every other APIs, different contexts were required
one and only one SparkContext on a single JVM

SparkSession
New entry point after 2.0
Includes all the APIs in different contexts - Spark Context, SQL Context, Streaming Context, Hive Context.
Can have zero, two or more SparkSessions in a single Spark application.
Have at least and often only one SparkSession in a Spark SQL application
SparkSession is a mere wrapper around SparkContext to offer Spark SQL's structured/SQL features on top of Spark Core's RDDs.


RDD: a collection of elements partitioned across the nodes of the cluster，可以进行并行化操作。一个machine上有几个partition，每个partition对应一个task，每个task可以由好几个core去执行。
DataFrame：基于RDD提供的，API上比RDD更容易使用，更适合工业界的实际场景，性能往往更好
Dataset：比DataFrame更高级的API，DataFrame 等于 Dataset[row]

shared variables：Spark runs a function in parallel as a set of tasks on different nodes, it ships a copy of each variable used in the function to each task
第一类：broadcast variables，which can be used to cache a value in memory on all nodes
第二类：accumulators, which are variables that are only “added” to

Spark SQL
Spark’s interface for working with structured data. Structured data is considered any data that has a schema.
1. It provides a DataFrame abstraction to simplify working with structured datasets. DataFrames are similar to tables in a relational database.
2. It can read and write data in a variety of structured formats (e.g., JSON, Hive Tables, and Parquet).
3. It lets you query the data using SQL, both inside a Spark program and from external tools that connect to Spark SQL through standard database connectors (JDBC/ ODBC), such as business intelligence tools like Tableau.


