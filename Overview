硬件层面
cluster
node
core：一个core就是一个cpu之类的计算单位，node上一般有好多core
master：控制node，用于分配任务和监控worker
worker：执行node，创建executor线程并执行计算

OS层面
Partition: 对数据的分区，每个partition对应一个task
Task：Each task is run as a process thread in executor JVM。每个task有自己的一个closure：属于自己的varible或method
Executor：Each Executor hosts 1 task during processing of a stage and the subsequent jobs associated with the code that was executed。

与硬件层面的联系：每个worker上有可以有好几个executor。


API层面
每一个spark程序都是由 a driver program that runs the user’s main function and executes various parallel operations on a cluster。

SparkContext
Entry point before 2.0
Create RDD and manipulate RDD API
For every other APIs, different contexts were required
one and only one SparkContext on a single JVM

SparkSession
New entry point after 2.0
Includes all the APIs in different contexts - Spark Context, SQL Context, Streaming Context, Hive Context.
Can have zero, two or more SparkSessions in a single Spark application.
Have at least and often only one SparkSession in a Spark SQL application
SparkSession is a mere wrapper around SparkContext to offer Spark SQL's structured/SQL features on top of Spark Core's RDDs.


RDD: a collection of elements partitioned across the nodes of the cluster，可以进行并行化操作。一个machine上有几个partition，每个partition对应一个task，每个task可以由好几个core去执行。
DataFrame：基于RDD提供的，API上比RDD更容易使用，更适合工业界的实际场景，性能往往更好
Dataset：比DataFrame更高级的API，DataFrame 等于 Dataset[row]

Shared variables：Spark runs a function in parallel as a set of tasks on different nodes, it ships a copy of each variable used in the function to each task
第一类：broadcast variables，which can be used to cache a value in memory on all nodes
第二类：accumulators, which are variables that are only “added” to

Spark SQL
Spark专门用来处理structured data的engine（不光是执行SQL语句）

Spark Streaming
用来处理RDD streaming的，执行在Spark engine上

Spark Structured Streaming
用来处理结构化数据 streaming的，执行在Spark SQL engine上

Library结构
pyspark
  RDD, Broadcast, Accumulator等关于RDD的class
  pyspark.sql SQL的subpackage，包括了Structured Streaming的功能（跟着DataFrame）
  pyspark.streaming DStream的subpackage
  pyspark.ml DataFrame-based machine learning APIs的subpackage
  pyspark.mllib RDD based API的subpackage


