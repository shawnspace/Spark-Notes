硬件层面
cluster
node
core：一个core就是一个cpu之类的计算单位

OS层面
Task：Each task is run as a process thread in executor JVM。每个task有自己的一个closure：属于自己的varible或method
Executor：Each Executor hosts multiple tasks during processing of a stage and the subsequent jobs associated with the code that was executed。
Partition: 对数据的分区
JVM：每个node上有一个JVM，一个JVM里有好几个executor。

关系：一个node上有几个partition，每个partition对应一个task，一个Executor执行好几个task，每个task可以由好几个core去执行。

API层面
每一个spark程序都是由 a driver program that runs the user’s main function and executes various parallel operations on a cluster。

SparkContext
Entry point before 2.0
Create RDD and manipulate RDD API
For every other APIs, different contexts were required
one and only one SparkContext on a single JVM

SparkSession
New entry point after 2.0
Includes all the APIs in different contexts - Spark Context, SQL Context, Streaming Context, Hive Context.
Can have zero, two or more SparkSessions in a single Spark application.
Have at least and often only one SparkSession in a Spark SQL application
SparkSession is a mere wrapper around SparkContext to offer Spark SQL's structured/SQL features on top of Spark Core's RDDs.


RDD: a collection of elements partitioned across the nodes of the cluster，可以进行并行化操作。一个machine上有几个partition，每个partition对应一个task，每个task可以由好几个core去执行。
DataFrame：基于RDD提供的，API上比RDD更容易使用，更适合工业界的实际场景，性能往往更好
Dataset：比DataFrame更高级的API，DataFrame 等于 Dataset[row]

Shared variables：Spark runs a function in parallel as a set of tasks on different nodes, it ships a copy of each variable used in the function to each task
第一类：broadcast variables，which can be used to cache a value in memory on all nodes
第二类：accumulators, which are variables that are only “added” to

Spark SQL
Spark专门用来处理structured data的engine（不光是执行SQL语句）


